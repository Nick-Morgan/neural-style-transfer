{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Use This Notebook\n",
    "\n",
    "### If using Google Colab (recommended)\n",
    "1.   Click `Runtime > Change runtime type` and set `Hardware accelerator` to GPU\n",
    "2.   Run the next 3 cells to load the repository and download the pre-trained models\n",
    "    - Note: If you get the following warning, ensure you uncheck \"Reset all runtimes before running\":\n",
    "    \n",
    "    ![warning](img/warning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If running notebook elsewhere:\n",
    "1.   I recommend using a GPU. You will not be able to keep up with the presentation otherwise.\n",
    "2.   If you have a Unix-based system, run the 3rd cell, which will download ~750 megabytes of pre-trained models into the models directory\n",
    "3.   If you do not have a Unix-based system, you may download the models manually, like a caveman, at this [link](https://drive.google.com/open?id=17Bm6A_lrONgfMqXGzAQ9moajL8zQYvX0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf sample_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Nick-Morgan/neural-style-transfer.git tmp; \\\n",
    " mv tmp/* .; rm -rf tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!helperFunctions/bash/downloadModels.sh > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import functools\n",
    "from textwrap import wrap\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing import image as kp_image\n",
    "from tensorflow.python.keras import models \n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = (10,10)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "\n",
    "Following the original [NST paper](https://arxiv.org/pdf/1508.06576.pdf), we will use the [VGG-19 Network](https://arxiv.org/pdf/1409.1556.pdf)\n",
    "\n",
    "![VGG19 - Clifford K. Yang](img/vgg19.jpg)\n",
    "\n",
    "The *Visual Geometry Group* (VGG) has published this pre-trained model, which gets its name from the 16 convolutional layers + 3 fully-connected layers. Its original purpose was image classification - it can detect 1,000 different objects in an image.\n",
    "\n",
    "We are not interested in image classification, however, because this model was trained on over 1-million images from the [ImageNet](http://www.image-net.org/) database, it can effectively separate low-level features (at the earlier layers) from the high level features (at the deeper layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![arxiv - 1508.06576 - Figure 1](img/1508.06576.fig-1.png)\n",
    "\n",
    "### Style Reconstructions:\n",
    "A feature space is built on top of the filter responses for each layer in \n",
    "\n",
    "Recall that a digital image is a 3-dimensional vector (traditionally RGB, but defined as BGR in this model). The output of each layer also produces a BGR vector. We compare the correlations between each of these layers to define the style. An image can then be reconstructed based on the representations built on subsets of CNN layers:\n",
    "\n",
    "**(a)**: conv1_1 \n",
    "\n",
    "**(b)**: conv1_1 and conv2_1 \n",
    "\n",
    "**(c)**: conv1_1, conv2_1 and conv3_1\n",
    "\n",
    "**(d)**: conv1_1, conv2_1, conv3_1 and conv4_1 \n",
    "\n",
    "**(e)**: conv1_1, conv2_1, conv3_1, conv4_1 and conv5_1\n",
    "\n",
    "### Content Reconstructions:\n",
    "The lower layers (**a, b, c**) reproduce seemingly identical pixels of the input image, whereas the higher layers (**d, e**) capture the *content* of the image, but do not constrain the exact pixel values.\n",
    "**(a)** - conv1_1, **(b)** - conv2_1 ,**(c)** - conv3_1, **(d)** - conv4_1, **(e)** - conv5_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styleDict = {\n",
    "    'a': ['conv1_1'],\n",
    "    'b': ['conv1_1', 'conv2_1'],\n",
    "    'c': ['conv1_1', 'conv2_1', 'conv3_1'],\n",
    "    'd': ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1'],\n",
    "    'e': ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']   \n",
    "}\n",
    "\n",
    "contentLayer = 'conv4_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow includes the vgg19 model under tf.keras.applications, however, the original NST paper made a few modifications. Namely, they do not use maxpooling, but instead use average pooling, which results in a better final image.\n",
    "\n",
    "log0 has created a [function](http://www.chioka.in/tensorflow-implementation-neural-algorithm-of-artistic-style) which modifies the vgg19 model to include average pooling. I have used his `load_vgg_model()` function, and made a modification that allows images of any size to be used. This function requires the pre-trained network as a .mat file, which can be downloaded [here](http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat). I did not include it in the repository because it is 500 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperFunctions.log0.vgg_config import load_vgg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Loss\n",
    "We want our generated image to have content which is similar to the content image. As such, we need to define a loss function so that the neural network knows how \"close\" it is to the original content image. \n",
    "\n",
    "Given a hidden layer (in this case, using block4_conv2), we take the sum of squared error - where the error is defined as the difference in activations between the content image and the generated image. Following the convention used in the original NST paper, we will define our loss as follows:\n",
    "\n",
    "\n",
    "$$J_{content}(C,G) =  \\frac{1}{4 \\times height \\times width \\times channels}\\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_content_cost(content_activations, generated_activations):    \n",
    "    \"\"\"\n",
    "    purpose:\n",
    "        For a **single layer's** activations, compute the cost between a generated\n",
    "        image and the original content image\n",
    "    \n",
    "    details:\n",
    "        - Following arXiv:1508.06576, compute the cost as defined in the equation above\n",
    "    args:\n",
    "        content_activations   (4D array)\n",
    "            - numpy representation of content image's activations\n",
    "            \n",
    "        generated_activations (4D array)\n",
    "            - numpy representation of generated image's activations\n",
    "    \n",
    "    rets:\n",
    "        Sum of squared errors, multiplied by (1/4 * height * width * channels)\n",
    "    \"\"\"\n",
    "    \n",
    "    m, height, width, channels = generated_activations.get_shape().as_list()\n",
    "    \n",
    "    return 1 / (4 * height * width * channels) *  tf.reduce_sum(tf.square(content_activations - generated_activations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Loss - Gram Matrices\n",
    "Recall that a [gram matrix](https://en.wikipedia.org/wiki/Gramian_matrix) is calculated by taking the dot product of a matrix transposed by itself. Andrew Ng has a helpful visualization in his coursera series on Convolutional Neural Networks:\n",
    "\n",
    "![Andrew Ng - Gram Matrix](img/andrew_ng_gram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(A):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        A: - matrix of shape (n_c, n_H*n_W)\n",
    "    rets:\n",
    "        Gram Matrix of A, shape (n_C, n_C)\n",
    "    \"\"\"\n",
    "    return tf.matmul(A, tf.transpose(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The style cost function is essentially the same as the content cost function, with two changes:\n",
    "\n",
    "1) The sum of squared error is now with regard to the two gram matrices\n",
    "\n",
    "2) The channels, height, and width in the denominator are squared\n",
    "\n",
    "$$J_{style}^{[l]}(S,G) = \\frac{1}{4 \\times {n_C}^2 \\times (n_H \\times n_W)^2} \\sum _{i=1}^{n_C}\\sum_{j=1}^{n_C}(G^{(S)}_{ij} - G^{(G)}_{ij})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_single_style_cost(style_activations, generated_activations):\n",
    "    \"\"\"\n",
    "    purpose:\n",
    "        For a **single** hidden layer's activations, compute the cost between a\n",
    "        generated image and the original style image\n",
    "\n",
    "    details:\n",
    "        - 1) Create a gram matrix for both activations\n",
    "        - 2) Compute the cost, as described above\n",
    "        \n",
    "    args:\n",
    "        style_activations   (4D array)\n",
    "            - numpy representation of content image's activations\n",
    "            \n",
    "        generated_activations (4D array)\n",
    "            - numpy representation of generated image's activations\n",
    "    \n",
    "    rets:\n",
    "        Sum of squared errors, multiplied by (1/4 * channels^2 * (height * width)^2 )\n",
    "    \"\"\"\n",
    "    \n",
    "    m, height, width, channels = generated_activations.get_shape().as_list()\n",
    "    \n",
    "    \n",
    "    # reshape into 2 dimensional matrix, then transform into gram matrix\n",
    "    style_activations = tf.transpose(tf.reshape(style_activations,\n",
    "                                               (height*width, channels)))\n",
    "    \n",
    "    generated_activations = tf.transpose(tf.reshape(generated_activations,\n",
    "                                                   (height*width, channels)))\n",
    "    \n",
    "    style_gram = gram_matrix(style_activations)\n",
    "    generated_gram = gram_matrix(generated_activations)\n",
    "    cost = 1 /(4*height*width*channels*height*width*channels)*(\n",
    "        tf.reduce_sum(tf.square(tf.subtract(style_gram,generated_gram))))\n",
    "        \n",
    "   \n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do this for multiple layers, we will create another function that takes the average cost across all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_style_cost(model, style_layers):\n",
    "    \"\"\"\n",
    "    purpose:\n",
    "        Compute the average style cost for a given model, across given layers\n",
    "\n",
    "    details:\n",
    "        - Set coefficient to 1 / (n layers)\n",
    "        - For each layer in style_layers, calculate the cost via\n",
    "        compute_single_style_cost(). Take the average and return\n",
    "        \n",
    "    args:\n",
    "        model - tensorflow model object\n",
    "        style_layers - list - which layers to use for the style\n",
    "    rets:\n",
    "        avg_cost - average cost value of all style activations\n",
    "       \n",
    "    \"\"\"\n",
    "    avg_cost = 0\n",
    "    coeff = 1 / len(style_layers)\n",
    "    for layer_name in style_layers:\n",
    "        \n",
    "        # filter to selected model\n",
    "        out = model[layer_name]\n",
    "\n",
    "        # create style activations and generated activations\n",
    "        style_activation = sess.run(out)\n",
    "        generated_activation = out\n",
    "\n",
    "        # Compute individual cost for that layer\n",
    "        individual_cost = compute_single_style_cost(style_activation, generated_activation)\n",
    "\n",
    "        # multiply cost * coefficient, which gives us the average for all layers\n",
    "        avg_cost += coeff * individual_cost\n",
    "\n",
    "    return avg_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we weight and combine the two cost functions to give us our overall loss:\n",
    "\n",
    "$$J(G) = \\alpha J_{content}(C,G) + \\beta J_{style}(S,G)$$\n",
    "\n",
    "Adjusting these weights will give us different outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_cost(content_loss, style_loss, alpha=10, beta=40):\n",
    "    \"\"\"\n",
    "    purpose:\n",
    "        Combine the content loss with the style loss, weighting accordingly\n",
    "        \n",
    "    args:\n",
    "        content_loss - tensorflow object representing content loss\n",
    "        style_loss   - tensorflow object representing style loss\n",
    "        alpha        - integer weight to apply to content_loss\n",
    "        beta         - integer weight to apply to style_loss\n",
    "    rets:\n",
    "        total_loss   - tensorflow object representing total loss\n",
    "       \n",
    "    \"\"\"\n",
    "    total_loss = alpha*content_loss + beta*style_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "\n",
    "If you've used Photoshop or similar software, you are probably used to the RGB convention of images. The [VGG Networks](https://arxiv.org/pdf/1409.1556.pdf) works with BGR instead. It also preprocesses the images by normalizing BGR through subtracting [103.939, 116.779, 123.68]. We will also need to add a 4th dimension.\n",
    "\n",
    "However, as you know, matplotlib plots images with 3 dimensions in RGB, so we will also need to create a function that transforms these images back to normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path_to_img, max_dim = 512):\n",
    "    \"\"\"\n",
    "    purpose: \n",
    "        Reshape an input image into dimensions compatible with VGG19 model\n",
    "    \n",
    "    details:\n",
    "        1) Load image\n",
    "        2) Resize to maximum dimension size\n",
    "        3) Store image in numpy array\n",
    "        4) Convert 3D image into 4D via np.expand()\n",
    "        \n",
    "    args:\n",
    "        path_to_img (string)    file location of image\n",
    "        max_dim     (integer)   maximum dimensions for rescaling large images\n",
    "        \n",
    "    rets:\n",
    "        img         (np array)  rescaled 4D image array\n",
    "    \"\"\"\n",
    "    img = Image.open(path_to_img)\n",
    "    long = max(img.size)\n",
    "    scale = max_dim/long\n",
    "\n",
    "    img = img.resize((round(img.size[0]*scale), round(img.size[1]*scale)), Image.ANTIALIAS)\n",
    "    img = kp_image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    return img        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_img(path_to_img):\n",
    "    \"\"\"\n",
    "    purpose:\n",
    "        apply vgg19 preprocessing to image\n",
    "    args:\n",
    "        path_to_img   (string)   file location of image\n",
    "    rets:\n",
    "        img           (np array) 4D representation of image\n",
    "    \"\"\"\n",
    "    img = load_img(path_to_img)\n",
    "    img = tf.keras.applications.vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_img(processed_img):\n",
    "    \"\"\"\n",
    "    purpose:\n",
    "        reverse vgg19 preprocessing\n",
    "    details:\n",
    "        -add [103.939, 116.779, 123.68] to respective BGR vectors\n",
    "        -remove 4th dimension from image\n",
    "    args:\n",
    "        processed_img (4D np array)\n",
    "    rets:\n",
    "        img (3D np array)\n",
    "    \"\"\"\n",
    "    img = processed_img.copy()\n",
    "    if len(img.shape) == 4:\n",
    "        img = np.squeeze(img, 0)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input to deprocessing image\")\n",
    "\n",
    "    img[:, :, 0] += 103.939 # Blue\n",
    "    img[:, :, 1] += 116.779 # Green\n",
    "    img[:, :, 2] += 123.68  # Red\n",
    "    img = img[:, :, ::-1]   # 4D to 3D\n",
    "\n",
    "    img = np.clip(img, 0, 255).astype('uint8')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to ensure that the content image and the style image are of the same dimensions. This is because we need the dimensions in the hidden layers to be the same in order to calculate the cost functions that were defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_images(content_image, style_image):\n",
    "    _, w, h, _ = content_image.shape\n",
    "    style_image = style_image[:,:w,:h,:] #reshape style\n",
    "    \n",
    "    _, w, h, _ = style_image.shape\n",
    "    content_image = content_image[:,:w,:h,:] #reshape content\n",
    "    return content_image, style_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to be generating many images, I've created a helper function to plot them along the specified axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img, ax, title=None):\n",
    "    \"\"\"\n",
    "    purpose:\n",
    "        -Remove 4th dimension from image array, plot via\n",
    "        matplotlib.pyplot.imshow()\n",
    "        \n",
    "    args:\n",
    "        img    (np array)    4D representation of image\n",
    "        ax     (plt Axes)    Location of plotted image\n",
    "        title  (string)      Title for plotted image\n",
    "    \"\"\"\n",
    "    out = np.squeeze(img, axis=0).astype('uint8')\n",
    "    ax.tick_params(labelbottom=False, labelleft=False)\n",
    "    ax.imshow(out)\n",
    "    if title:\n",
    "        ax.set_title(title)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are defining our cost function for both style and content by comparing to a generated image. We need a starting point for this image that is similar to the original content, but slightly different.\n",
    "\n",
    "In order to accomplish that, we can add noise to our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_image(image, ratio):\n",
    "    \"\"\"\n",
    "    purpose:\n",
    "        add ratio % noise to an image\n",
    "    details:\n",
    "        - create a random numpy array with same dimensions as input image\n",
    "        - add (noise * ratio) + (image * 1-ratio)\n",
    "    args:\n",
    "        image (np array) original image\n",
    "        ratio (float)    decimal representation of % noise to add\n",
    "    rets:\n",
    "        (np array) image with noise added\n",
    "    \n",
    "    \"\"\"\n",
    "    noise = np.random.uniform(-20, 20, (image.shape)).astype('float32')\n",
    "    \n",
    "    return (noise * ratio) + (image * (1 - ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples of what this noise looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_example = load_img('data/content/stonearch.jpg')\n",
    "noise_pct = 0.0\n",
    "fig, ax = plt.subplots(3, 3, figsize=(10,10))\n",
    "for x in range(3):\n",
    "    for y in range(3):\n",
    "        noisy_img = add_noise_to_image(noise_example, noise_pct)\n",
    "        plot_image(noisy_img, ax[x][y], '{0:.0%} Noise'.format(noise_pct))\n",
    "        noise_pct+= 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the percentage of noise in the original generated image can have drastic effects on the output. I demonstrate this later on, but the majority of examples I've created use 60% noise, as I've found that tends to have the most appealing results.\n",
    "\n",
    "\n",
    "Now we can combine everything into a final function to generate our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer(\n",
    "             sess,\n",
    "             content_path,\n",
    "             style_path,\n",
    "             model_path,\n",
    "             name,\n",
    "             contentLayer='conv4_2',\n",
    "             styleLayers=['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1'],\n",
    "             noise_ratio=0.6,       \n",
    "             num_iterations = 200):\n",
    "    \n",
    "    \"\"\"\n",
    "    purpose:\n",
    "        -Apply neural style transfer from a style image onto a content image,\n",
    "        in order to create a generated image\n",
    "        \n",
    "    details:\n",
    "        1) Load an preprocess content image and style image according to vgg19 \n",
    "        specifications\n",
    "        \n",
    "        2) Generate noisy image by using noise_ratio % of noise\n",
    "        \n",
    "        3) Load the vgg model provided by log0, and configure based on content image shape\n",
    "        \n",
    "        4) Combine weighted content cost & style cost, and then optimize via the\n",
    "        AdamOptimizer in order to minimize this total cost. \n",
    "        \n",
    "        5) Continue to minimize the total cost, updating values each iteration and\n",
    "        re-calculating the loss.\n",
    "        \n",
    "        6) Each 20 iterations, store a snapshot of the image in output/\n",
    "        \n",
    "        7) Store the final image in output/\n",
    "        \n",
    "    args:\n",
    "        sess -           tensorflow object - current tensorflow session\n",
    "        content_path -   string -            file location of content image\n",
    "        style_path -     string -            file location of style image\n",
    "        model_path -     string -            file location of .mat model file\n",
    "        name -           string -            name of output file\n",
    "        contentLayer -   string -            which layer to use to define image content\n",
    "        styleLayers -    list -              which **layers** to use to define style content\n",
    "        noise_ratio -    float -             % of noise to use for initial generated image      \n",
    "        num_iterations - integer -           number of epochs to use\n",
    "    rets:\n",
    "        generated_image- (4D np array) -      final generated image\n",
    "        \n",
    "    notes:\n",
    "        The final image and all intermediate epoch images will be stored\n",
    "        in data/generated/gatys/\n",
    "    \"\"\"    \n",
    "    # Load images, resize, generate noisy image\n",
    "    content_image = load_img(content_path).astype('uint8')\n",
    "    style_image = load_img(style_path).astype('uint8')\n",
    "    content_image, style_image = reshape_images(content_image, style_image)\n",
    "    noisy_image = add_noise_to_image(content_image, ratio=noise_ratio)\n",
    "    \n",
    "    # load vgg model with specified dimensions, compute initial cost function\n",
    "    vgg = load_vgg_model(model_path, content_image.shape)\n",
    "    sess.run(vgg['input'].assign(content_image))\n",
    "\n",
    "    content_activation = sess.run(vgg[contentLayer])\n",
    "    generated_activation = vgg[contentLayer] # placeholder for now. will be updated later\n",
    "    content_cost = compute_content_cost(content_activation, generated_activation)\n",
    "\n",
    "    sess.run(vgg['input'].assign(style_image))\n",
    "    style_cost = compute_avg_style_cost(vgg, styleLayers)\n",
    "\n",
    "    total_cost = compute_total_cost(content_cost, style_cost,\n",
    "                                    alpha=10, beta=40)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(2.0)\n",
    "    train_step = optimizer.minimize(total_cost)\n",
    "    \n",
    "\n",
    "    # initialize global variables, run the noisy image, loop through epochs\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(vgg['input'].assign(noisy_image))\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        sess.run(train_step)\n",
    "        generated_image = sess.run(vgg['input'])\n",
    "       # Print every 20 iteration.\n",
    "        if i%20 == 0:\n",
    "            Jt, Jc, Js = sess.run([total_cost, content_cost, style_cost])\n",
    "            print(\"Iteration \" + str(i) + \" :\")\n",
    "            print(\"total cost = \" + str(Jt))\n",
    "            print(\"content cost = \" + str(Jc))\n",
    "            print(\"style cost = \" + str(Js))\n",
    "\n",
    "            # save current generated image in the \"/output\" directory\n",
    "            path = 'data/generated/gatys/{0}_{1}.png'.format(name, i)\n",
    "            cv2.imwrite(path, deprocess_img(generated_image))\n",
    "\n",
    "    # save last generated image\n",
    "    path = 'data/generated/gatys/{0}_final.png'.format(name, i)\n",
    "    cv2.imwrite(path, deprocess_img(generated_image))\n",
    "\n",
    "    return generated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This next cell will take quite a long time if it is not run on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "run_style_transfer(sess,\n",
    "         content_path='data/content/stonearch.jpg',\n",
    "         style_path='data/style/rain_princess.jpg',\n",
    "         model_path='models/imagenet-vgg-verydeep-19.mat',\n",
    "         contentLayer='conv4_2',\n",
    "         styleLayers=styleDict['e'],\n",
    "         noise_ratio=0.6,       \n",
    "         name='stonearch',\n",
    "         num_iterations = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Here is the original content image that has been transformed:\n",
    "![Stonearch Bridge](data/content/stonearch.jpg)\n",
    "\n",
    "This image was taken by Mac H (media601) on flickr, and this image has been labelled for re-use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these images were generated using `styleDict['e']`, with a noise percentage of 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (30,30)\n",
    "styleDir = 'data/style/'\n",
    "generatedDir = 'data/generated/gatys/'\n",
    "styleList = os.listdir(styleDir)\n",
    "\n",
    "fig, ax = plt.subplots(len(styleList), 3, figsize=(15, 35))\n",
    "\n",
    "x = 0\n",
    "for fp in styleList:\n",
    "    original = load_img(styleDir + fp)\n",
    "    epoch_80 = load_img('{0}stonearch_{1}_80.png'.format(generatedDir, fp[:-4]))\n",
    "    final = load_img('{0}stonearch_{1}_final.png'.format(generatedDir, fp[:-4]))\n",
    "    \n",
    "    plot_image(original, ax[x][0], 'Style Image')\n",
    "    plot_image(epoch_80, ax[x][1], '80 Epochs')\n",
    "    plot_image(final, ax[x][2], '200 Epochs')\n",
    "    \n",
    "    x+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of noise\n",
    "I generated the same style image, with an original noise percentage of 0.1, 0.3, 0.5, 0.7, and 0.9 values of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,3,figsize=(15,35))\n",
    "\n",
    "original = load_img('data/style/rain_princess.jpg')\n",
    "x = 0\n",
    "for noise_pct in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    epoch_20 = load_img('data/generated/gatys/stonearch_rain_princess_{0}_noise_20.png'.format(noise_pct))\n",
    "    final = load_img('data/generated/gatys/stonearch_rain_princess_{0}_noise_final.png'.format(noise_pct))\n",
    "    \n",
    "    plot_image(original, ax[x][0], 'Style Image')\n",
    "    plot_image(epoch_20, ax[x][1], '20 Epochs\\n{0:.0%} Noise'.format(noise_pct))\n",
    "    plot_image(final, ax[x][2], '200 Epochs\\n{0:.0%} Noise'.format(noise_pct))\n",
    "    \n",
    "    x+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects of Style Layers\n",
    "Recall that the style cost is calculated by averaging the difference in activations at each layer. Also recall the various layers that were used in the original article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styleDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied the style transfer using these various layers for style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,3,figsize=(15,35))\n",
    "\n",
    "original = load_img('data/style/rain_princess.jpg')\n",
    "x = 0\n",
    "for layers in styleDict.keys():\n",
    "    subTitle = '\\n'.join(wrap(', '.join(styleDict[layers]), width=30))\n",
    "    \n",
    "    epoch_20 = load_img('data/generated/gatys/stonearch_rain_princess_{0}_20.png'.format(layers))\n",
    "    final = load_img('data/generated/gatys/stonearch_rain_princess_{0}_final.png'.format(layers))\n",
    "    \n",
    "    plot_image(original, ax[x][0], 'Style Image')\n",
    "    plot_image(epoch_20, ax[x][1], '20 Epochs\\nLayers: {0}'.format(subTitle))\n",
    "    plot_image(final, ax[x][2], '200 Epochs\\nLayers: {0}'.format(subTitle))\n",
    "    \n",
    "    x+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Style Transfer\n",
    "\n",
    "The method introduced by Gatys is somewhat slow, because inference requires solving an optimization problem for each image. An [alternate method](https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf) trains a network to understand the transformation between a style image and [~100,000 content images](https://arxiv.org/pdf/1405.0312.pdf). \n",
    "\n",
    "\n",
    "Once this network has been trained, it can transform a new content image in a single forward pass. Minimizing the same cost function as Gatys, a single forward pass in Johnson's method results in a score that is equivalent of 100-150 epochs from Gatys' method.\n",
    "\n",
    "\n",
    "![JohnsonECCV16 - Figure 2](img/eccv16-fig-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This system consists of two components: an *image transformation network fw* and a *loss network Ï†* that is used to define several loss functions. The image transformation network (deep residual CNN) transforms input images into output images by mapping *y = fW (x)*. The image transformation network is trained using stochastic gradient descent to minimize a weighted combination of loss functions:\n",
    "![JohnsonECCV16 - Section 3, Function 1](img/eccv16-func-3-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Transformation Architecture ([link](https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16Supplementary.pdf)):\n",
    "![JohnsonECCV16Supplementary - Fig 1](img/eccv16-supplemental-fig-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A consequence of the downsampling > residual blocks > upsampling is that the content image becomes more generalized, and can capture larger concepts of an image instead of individual pixels:\n",
    "\n",
    "![Source: https://cs.stanford.edu/people/jcjohns/eccv16/](img/eccv16-la-muse-comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major theme of *The Muse* is the presence of triangles. Johnson's method more effectively captures this theme, and also captures more of the texture. Gatys' method does still capture the style, but it appears to capture more *local* styles than *global* styles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each style must be trained on ~100,000 content images, training can take quite a long time. 2 epochs take between 2-4 hours on an AWS p3.8xlarge server. Fortunately, [Logan Engstrom](https://github.com/lengstrom/fast-style-transfer) has pre-trained 6 models using this method (albeit with VGG19 instead of VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperFunctions.lengstrom import transform, vgg\n",
    "from helperFunctions.lengstrom.utils import save_img, get_img, exists, list_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffwd(content_path, model_path, name, device_t='/gpu:0'):\n",
    "    \"\"\"\n",
    "    purpose:\n",
    "        -Using a model that was trained via Johnson's method, transform a content\n",
    "        image into a generated image with the respective style applied\n",
    "        \n",
    "    args:\n",
    "        content_path -   string -            file location of content image\n",
    "        model_path -     string -            file location of .ckpt model file\n",
    "        name -           string -            name of output file\n",
    "        device_t -       string -            device used to transform image\n",
    "\n",
    "    rets:\n",
    "        Nothing\n",
    "        \n",
    "    notes:\n",
    "        The generated image will be stored in data/generated/gatys/\n",
    "    \"\"\" \n",
    "    X = np.expand_dims(get_img(content_path), axis=0)\n",
    "    output_path = 'data/generated/johnson/{0}.png'.format(name)\n",
    "    \n",
    "    g = tf.Graph()\n",
    "    soft_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    soft_config.gpu_options.allow_growth = True\n",
    "    with g.as_default(), g.device(device_t), \\\n",
    "            tf.Session(config=soft_config) as sess:\n",
    "\n",
    "        img_placeholder = tf.placeholder(tf.float32, shape=X.shape,\n",
    "                                         name='img_placeholder')\n",
    "\n",
    "        # load model, transform image\n",
    "        preds = transform.net(img_placeholder)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path)\n",
    "        _preds = sess.run(preds, feed_dict={img_placeholder:X})\n",
    "        \n",
    "        # save\n",
    "        save_img(output_path, _preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffwd(content_path='data/content/stonearch.jpg',\n",
    "     model_path='models/udnie.ckpt',\n",
    "     name='stonearch_udnie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (30,30)\n",
    "styleDir = 'data/style/'\n",
    "generatedDir = 'data/generated/'\n",
    "\n",
    "styleList = os.listdir(styleDir)\n",
    "styleList.remove('great_wave_remix_randal_roberts_web.jpg')\n",
    "\n",
    "x = 0\n",
    "\n",
    "fig, ax = plt.subplots(6,3,figsize=(15,35))\n",
    "for fp in styleList:\n",
    "    style = load_img('data/style/{0}.jpg'.format(fp[:-4]))\n",
    "    gatys = load_img('data/generated/gatys/stonearch_{0}_final.png'.format(fp[:-4]))\n",
    "    johnson = load_img('data/generated/johnson/stonearch_{0}.png'.format(fp[:-4]))\n",
    "    \n",
    "    plot_image(style, ax[x][0], 'Style Image')\n",
    "    plot_image(gatys, ax[x][1], 'Gatys - 200 Epochs')\n",
    "    plot_image(johnson, ax[x][2], 'Johnson - 1 Forward Pass')\n",
    "    \n",
    "    x+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources:\n",
    "\n",
    "### Content:\n",
    "Stonearch Bridge - Mac H (media601) on flickr\n",
    "\n",
    "### Style:\n",
    "Rain Princess - Leonid Afremov\n",
    "\n",
    "The Scream - Edvard Munch\n",
    "\n",
    "The Shipwreck of the Minotaur - Joseph Mallord William Turner\n",
    "\n",
    "Udnie - Francis Picabia\n",
    "\n",
    "The Great Wave off Kanagawa - Hokusai\n",
    "\n",
    "The Great Wave Remix - Randal Roberts\n",
    "\n",
    "\n",
    "### Code:\n",
    "[Neural Style Transfer - Raymond Yuan](https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398)\n",
    "\n",
    "[Tensorflow Implementation Neural Algorithm of Artistic Style - Log0](http://www.chioka.in/tensorflow-implementation-neural-algorithm-of-artistic-style)\n",
    "\n",
    "[Fast Style Transfer - Logan Engstrom](https://github.com/lengstrom/fast-style-transfer)\n",
    "\n",
    "### Theory:\n",
    "[A Neural Algorithm of Artistic Style](https://arxiv.org/pdf/1508.06576.pdf)\n",
    "\n",
    "[Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:foo]",
   "language": "python",
   "name": "conda-env-foo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
